{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "10주차출석과제(오차역전파법).ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNHBY7PNOUIAIBbWBNljJSE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mapbefun/mapbefun/blob/main/10%EC%A3%BC%EC%B0%A8%EC%B6%9C%EC%84%9D%EA%B3%BC%EC%A0%9C(%EC%98%A4%EC%B0%A8%EC%97%AD%EC%A0%84%ED%8C%8C%EB%B2%95).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NqrPMRyasSlJ"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hJaVpRbSBna"
      },
      "source": [
        "제 5장 오차역전파법_03 & 제 5장 오차역전파법_04를 듣고 아래를 파이썬을 이용하여 구현하고 결과를 분석하시오.<br>\n",
        "1.함수 softmax, cross_entropy_error를 정의한다.  동영상 03강의 참조<br>\n",
        "2.클래스 Relu, Sigmoid, Affine, SoftmaxWithLoss를 정의한다. 이들 클래스는 __init__, forward, backward의 함수를 지닌다. 동영상 03강의 참조<br>\n",
        "3.클래스 ANN을 정의한다. 클래스내에 __init__, predict, accuracy, loss, gradient, update, fit 함수를 정의한다. 동영상 03강의 참조\n",
        "4.우리가 만든 MNIST 데이터셋의 80%는 훈련용 20%는 검증용으로 만든다.<br>\n",
        "5.MNIST 데이터셋을 입력받는다. 이때 One_Hot_Encoded data를 입력한다.\n",
        "6.히든레이어는 1000, 500, 250, 125, 60, 30으로 만들어 학습시킨 후, 검증용 데이터를 이용한 정확도를 계산한다. (우리가 만든 MNIST 검증데이터와 MNIST 검증데이터 각각의 정확도를 계산)<br>\n",
        "7.회전을 이용하여 데이터를 확장한 후, 다시 학습시킨 후, 검증용 데이터를 이용한 정확도를 계산한다.(우리가 만든 MNIST 검증데이터와 MNIST 검증데이터 각각의 정확도를 계산)<br>\n",
        "8.확장한 데이터와 MNIST데이터를 통합한 후, 다시 학습시킨 후, 검증용 데이터를 이용한 정확도를 계산한다.(우리가 만든 MNIST 검증데이터와 MNIST 검증데이터 각각의 정확도를 계산)<br>\n",
        "9.위를 Keras로 구현한다.<br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4YW02_d-Nz2y"
      },
      "source": [
        "def cross_entropy_error(y,t):  #크로스엔트로피 함수를 정의한다. x값을 넣으면 Prediction해야 하므로 Prediction 된 y값을 넣는다. \n",
        "  if y.size == t.size:\n",
        "    t = np.argmax(t,axis=1)  #가로 축에서 어느 값이 최대값을 갖는지를 보기 위해 argmax함수를 쓴다. \n",
        "\n",
        "  n_size = y.shape[0]   #데이터의 사이즈를 정의: 0번째 데이터가 몇 개들어 왔는지\n",
        "\n",
        "  return -np.sum(np.log(y[np.arange(n_size),t]+1e-7))/n_size "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "coQvwORKi5BE"
      },
      "source": [
        "#분류하기 위해 소프트맥스 함수를 정의한다(이차원으로)\n",
        "def softmax(a):\n",
        "    if a.ndim == 2:\n",
        "        a=a.T\n",
        "        _max=np.max(a,axis=0)\n",
        "        exp_a=np.exp(a-_max) #a에서 max값을 빼준다\n",
        "        y=exp_a/np.sum(exp_a,axis=0)\n",
        "        return y.T\n",
        "    else: \n",
        "            _max=np.max(a)\n",
        "            exp_a=np.exp(a-_max)\n",
        "            y=exp_a/np.sum(exp_a)\n",
        "            return y  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBv-AElri9Lj"
      },
      "source": [
        "class SoftmaxWithLoss:\n",
        "    def __init__(self):\n",
        "        self.loss=None\n",
        "        self.y=None\n",
        "        self.t=None\n",
        "\n",
        "    def forward(self,x,t):\n",
        "        self.t=t\n",
        "        self.y=softmax(x)\n",
        "        self.loss=cross_entropy_error(self.t)\n",
        "        return self.loss\n",
        "\n",
        "    def backward(self,dout):\n",
        "        dx=(self.y-self.t)/self.t.shape[0]\n",
        "        return dx\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yIqnIwsqm22z"
      },
      "source": [
        "\n",
        "# 2층 신경망\n",
        "# __init__ --> 파라미터를 초기화\n",
        "# predict  --> 추론\n",
        "# accuracy --> 정확도\n",
        "# loss     --> 손실함수\n",
        "# gradient --> 경사도 방향벡터\n",
        "# update   --> 파라미터 갱신\n",
        "# fit      --> 학습 데이터..  -> gradient -> update -> gradient -> update... (학습)\n",
        "\n",
        "from collections import OderedDict\n",
        "\n",
        "class ANN:\n",
        "  def __init__(self,input_size,hidden_size,output_size,learning_rate,batch_size):\n",
        "\n",
        "    self.lr = learning_rate\n",
        "    self.batch_size = batch_size\n",
        "\n",
        "    self.param = {}\n",
        "    self.grad = {}\n",
        "\n",
        "    if not isinstance(hidden_size,list):\n",
        "      hidden_size = [hidden_size]\n",
        "\n",
        "    nNeuron = [input_size] + hidden_size + [output_size]\n",
        "    self.nLayer = len(nNeuron)-1\n",
        "\n",
        "    self.layers = OderedDict  # (딕셔너리가 있는) 연산레이어 생성\n",
        "\n",
        "\n",
        "    for i in range(self.nLayer):\n",
        "      self.param['W'+str(i+1)] = np.sqrt(2/nNeuron[i])*np.random.randn(nNeuron[i],nNeuron[i+1])\n",
        "      self.param['b'+str(i+1)] = np.zeros(nNeuron[i+1])\n",
        "      self.layers['Affine'+Str(i+1)]=Affine(self.param['W'+str(i+1)],self.parm['b'+str(i+1)])\n",
        "\n",
        "      if i !=self.nLayer -1:\n",
        "          self.layer['relu'+str(i+1)] = Relu()\n",
        "      else:\n",
        "          self.lastLayer = SofmaxLoss()\n",
        "\n",
        "\n",
        "  def predict(self,x):\n",
        "   for layer in self.layer.values():\n",
        "       x=layer.forward(x)\n",
        "   return x     \n",
        "\n",
        "# accuracy함수는 검증을 위한 함수임\n",
        "\n",
        "  def accuracy(self,x,t):\n",
        "    batch_size = self.batch_size\n",
        "    acc = 0\n",
        "    for i in np.arane(0,x.shape[0],batch_size):\n",
        "      y = self.predict(x[i:i+batch_size])\n",
        "\n",
        "      if y.size == t[i:i+batch_size].size:\n",
        "        pt = np.argmax(t[i:i+batch_size],axis=1)\n",
        "      else:\n",
        "        pt = t[i:i+batch_size]\n",
        "\n",
        "      p = np.argmax(y,axis=1)\n",
        "\n",
        "      acc+= np.sum(p==pt)\n",
        "\n",
        "    return axx/x.shape[0]\n",
        "\n",
        "\n",
        "  def loss(self,x,t):\n",
        "    y = self.predict(x)\n",
        "    return self.lastLayer.forward(y,t)\n",
        "\n",
        "  def gradient(self,x,t):\n",
        "    _loss = self.loss(x,t)\n",
        "\n",
        "    #backward\n",
        "    dout = 1 \n",
        "    dout = self.lastLayer.backward(dout)\n",
        "\n",
        "    layers = list(self.layers.values())\n",
        "    layers.reverse()\n",
        "    for layer in layers:\n",
        "        dout = layer.backward(dout)\n",
        "    return _loss \n",
        "\n",
        "\n",
        "  def update(self):\n",
        "    for i in range(self.nLayer):\n",
        "      self.layers['Affine'+str(i+1).W -= self.lr*self.layers['Affine'+str(i+1)].dW\n",
        "      self.param['Affine'+str(i+1).b -= self.lr*self.layers['Affine'+str(i+1)].dW\n",
        "\n",
        "\n",
        "  def fit(self,xTrain,tTrain,xTest,tTest,epoch):\n",
        "\n",
        "    for i in range(epoch):\n",
        "      vec = np.arange(xTrain.shape[0])\n",
        "      np.random.shuffle(vec)\n",
        "      xTrain = xTrain[vec]\n",
        "      tTrain = tTrain[vec]\n",
        "    \n",
        "      nBatch = xTrain.shape[0]//self.batch_size\n",
        "\n",
        "      for j in range(nBatch):\n",
        "        x = xTrain[j*self.batch_size:(j+1)*self.batch_size]\n",
        "        t = tTrain[j*self.batch_size:(j+1)*self.batch_size]\n",
        "\n",
        "\n",
        "        #gradient 구하기\n",
        "        _loss = self.gradient(x,t)\n",
        "        #update 하기\n",
        "        self.update()\n",
        "      \n",
        "        print(f\"{j}번째 업데이트: 손실값 {_loss}\")\n",
        "\n",
        "    print(\"\")\n",
        "    print(f\"{i}번째 epoch LOSS: {_loss}, 정확도: {self.accuracy(xTest,tTest)}\")\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UBj2Xt0vx3Q4"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/datalab/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w5m367eGx4ux"
      },
      "source": [
        "import sys, os\n",
        "sys.path.append(os.pardir)\n",
        "sys.path.append('/datalab/drive/MyDrive/딥러닝수업')\n",
        "my_path = '/datalab/drive/MyDrive/딥러닝수업/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCRqMFY3yaY2"
      },
      "source": [
        "import pickle\n",
        "\n",
        "with open(my_path+\"OurMNIST/OurMNISTdata.pkl\" ,\"rb\") as f:\n",
        "  our_images=pickle.load(f)\n",
        "with open(my_path+\"OurMNIST/OurMNISTlabel.pkl\" ,\"rb\") as f:\n",
        "  our_labels=pickle.load(f)\n",
        "\n",
        "print(our_images.shape)\n",
        "print(our_labels.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o7OJywPmyvHT"
      },
      "source": [
        "# one_hot_encoding\n",
        "def one_hot_encoder(data,nclass):\n",
        "    rows = np.zeros((data.shape[0]))\n",
        "    one_hot = np.zeros((data.shape[0],nclass),dtype=\"int32\")\n",
        "    one_hot[rows,data]=1\n",
        "    return one_hot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ECmovkFZ0H70"
      },
      "source": [
        "our_labels = one_hot_encoder(our_labels,10)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s9BCsqQ318DY"
      },
      "source": [
        "out_images_1d = our_images.reshape(-1,28*28)\n",
        "pos = []\n",
        "for i,img in enumerate(our_images_1d):\n",
        "    if np.sum(np.nan(img)):"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
